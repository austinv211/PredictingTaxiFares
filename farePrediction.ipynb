{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import our dataset\n",
    "- our dataset consists of several files:\n",
    "    - train.csv: our training data\n",
    "    - test.csv: our testing data\n",
    "    - sample_submissions.csv: A sample submission file in the correct format (columns key and fare_amount). This dummy file 'predicts' fare_amount to be $11.35 for all rows, which is the mean fare_amount from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vargasona/opt/anaconda3/lib/python3.7/site-packages/tqdm/std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed # 1\n",
      "completed # 2\n",
      "completed # 3\n",
      "completed # 4\n",
      "completed # 5\n",
      "completed # 6\n",
      "completed # 7\n",
      "completed # 8\n",
      "completed # 9\n",
      "completed # 10\n",
      "completed # 11\n",
      "completed # 12\n",
      "completed # 13\n",
      "completed # 14\n",
      "completed # 15\n",
      "completed # 16\n",
      "completed # 17\n",
      "completed # 18\n",
      "completed # 19\n",
      "completed # 20\n",
      "completed # 21\n",
      "completed # 22\n",
      "completed # 23\n",
      "completed # 24\n",
      "completed # 25\n",
      "completed # 26\n",
      "completed # 27\n",
      "completed # 28\n",
      "completed # 29\n",
      "completed # 30\n",
      "completed # 31\n",
      "completed # 32\n",
      "completed # 33\n",
      "completed # 34\n",
      "completed # 35\n",
      "completed # 36\n",
      "completed # 37\n",
      "completed # 38\n",
      "completed # 39\n",
      "completed # 40\n",
      "completed # 41\n",
      "completed # 42\n",
      "completed # 43\n",
      "completed # 44\n",
      "completed # 45\n",
      "completed # 46\n",
      "completed # 47\n",
      "completed # 48\n",
      "completed # 49\n",
      "completed # 50\n",
      "completed # 51\n",
      "completed # 52\n",
      "completed # 53\n",
      "completed # 54\n",
      "completed # 55\n",
      "completed # 56\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'pyarrow'.  Use pip or conda to install pyarrow.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8854db1b13a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# feather the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mfeather_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{DATA_FILES_PATH}train.feather'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# import the dataset from a feather\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-8854db1b13a6>\u001b[0m in \u001b[0;36mfeather_dataset\u001b[0;34m(dataframe, file_out)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfeather_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_feather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_feather\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m   2135\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeather_format\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_feather\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2137\u001b[0;31m         \u001b[0mto_feather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2139\u001b[0m     def to_parquet(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/feather_format.py\u001b[0m in \u001b[0;36mto_feather\u001b[0;34m(df, path)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \"\"\"\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyarrow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyarrow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeather\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py\u001b[0m in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, raise_on_missing, on_version)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Missing optional dependency 'pyarrow'.  Use pip or conda to install pyarrow."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import mmap\n",
    "\n",
    "DATA_FILES_PATH = 'projectDataFiles/'\n",
    "\n",
    "def import_training_dataset_limit(file_path, row_limit=100000):\n",
    "    \"\"\"\n",
    "    function to import the dataset into a pandas dataframe.\n",
    "\n",
    "    Takes a row limit to limit the number of rows read.\n",
    "    \"\"\"\n",
    "    if row_limit:\n",
    "        return pd.read_csv(file_path, nrows=row_limit)\n",
    "    else:\n",
    "        return pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "def import_training_dataset_chunked(file_path, chunksize=1000000):\n",
    "    \"\"\"\n",
    "    function to import the dataset into a pandas dataframe, reading the file in chunks and appending as we go\n",
    "    \"\"\"\n",
    "    tqdm.pandas(desc=\"Applying Transformation\")\n",
    "    df = pd.DataFrame()\n",
    "    counter = 0\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize, low_memory=True):\n",
    "        df = pd.concat([df, chunk])\n",
    "        counter += 1\n",
    "        print(f'completed # {counter}')\n",
    "    return df\n",
    "\n",
    "def read_feathered_data(file_path):\n",
    "    return pd.read_feather(file_path)\n",
    "\n",
    "def feather_dataset(dataframe, file_out):\n",
    "    dataframe.to_feather(file_out)\n",
    "        \n",
    "\n",
    "# assign the dataset to the TRAIN Dataframe, right now we are only loading 1,000,000 rows (possibly chunk and feather to reduce loading time)\n",
    "# TRAIN = import_training_dataset_limit(f'{DATA_FILES_PATH}train.csv')\n",
    "\n",
    "# import the dataset in chunks\n",
    "#TRAIN = import_training_dataset_chunked(f'{DATA_FILES_PATH}train.csv')\n",
    "\n",
    "# feather the dataset\n",
    "#feather_dataset(TRAIN, f'{DATA_FILES_PATH}train.feather')\n",
    "\n",
    "# import the dataset from a feather\n",
    "TRAIN = read_feathered_data(f'{DATA_FILES_PATH}train.feather')\n",
    "\n",
    "# show the head of the the dataset to see its columns\n",
    "TRAIN.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to get the manhattan distance between two lat/long points\n",
    "- manhattan distance should be fairly relistic to new york because of the ways that streets work there, but we might want to use the real travel distance between locations somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_manhattan_distance(lat1, lat2, long1, long2):\n",
    "    \"\"\"\n",
    "    returns the manhattan distance between two geo points\n",
    "    \"\"\"\n",
    "    return abs(lat2 - lat1) + abs(long2 - long1)\n",
    "\n",
    "\n",
    "# test it out\n",
    "geo_manhattan_distance(TRAIN['pickup_latitude'].iloc[0], TRAIN['dropoff_latitude'].iloc[0],\\\n",
    "TRAIN['pickup_longitude'].iloc[0], TRAIN['dropoff_longitude'].iloc[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to get the real distance between to lat/long points\n",
    "- Manhattan distance should be useful, but I think we can do better with real distance\n",
    "- Here we compare a manual calculation to the geopy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin, cos, sqrt, atan2, radians\n",
    "import geopy.distance\n",
    "\n",
    "def real_distance(lat1, lat2, long1, long2):\n",
    "    \"\"\"\n",
    "    returns the real distance between two datapoints\n",
    "    \"\"\"\n",
    "    R = 6373.0 #approximate radius of earth in km\n",
    "    rad_lat1, rad_lat2, rad_long1, rad_long2 = (radians(abs(meas)) for meas in [lat1, lat2, long1, long2])\n",
    "    long_dist = rad_long2 - rad_long1\n",
    "    lat_dist = rad_lat2 - rad_lat1\n",
    "    a = sin(lat_dist / 2)**2+ cos(rad_lat1) * cos(rad_lat2) * sin(long_dist / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "def geopy_dist(coord1, coord2):\n",
    "    return geopy.distance.distance(coord1, coord2).kilometers\n",
    "\n",
    "# test out both functions\n",
    "dist_test_1 = real_distance(TRAIN['pickup_latitude'].iloc[0], TRAIN['dropoff_latitude'].iloc[0],\\\n",
    "TRAIN['pickup_longitude'].iloc[0], TRAIN['dropoff_longitude'].iloc[0])\n",
    "\n",
    "dist_test_2 = geopy_dist((TRAIN['pickup_latitude'].iloc[0], TRAIN['pickup_longitude'].iloc[0]), (TRAIN['dropoff_latitude'].iloc[0], TRAIN['dropoff_longitude'].iloc[0]))\n",
    "\n",
    "print(f'Manual: {dist_test_1} km\\nGeopy: {dist_test_2} km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}